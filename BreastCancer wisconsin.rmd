---
title: "Analysis of the Breast Cancer Dataset"
author: "Neha Dixit, Ravindra Thanniru, Saloni Bhatia, Sangrae Cho"
date: "12/2/2019"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r requirments, message=FALSE, include=FALSE, echo=FALSE}
# Importing required libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(glmnet)
library(randomForest)
library(gbm)
library(rpart)
library(boot)
library(data.table)
library(ROCR)
library(gridExtra)
library(ResourceSelection)

# library imports
library(tidyverse)
# Date manipulation
library(lubridate)
# Plotting
library(olsrr)
# RMLSE
library(MLmetrics)
# Correlation Matrix
library(ggcorrplot)
# Lasso
library(glmnet)
# AIC commands
library(MASS)
#library(devtools)
library(tidyverse)
library("tree")
library(gridExtra)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot) 
library(e1071)
library(ggthemes)
library(caret)
library(tidyverse)
library(scales)
library("openxlsx")

# Importing the data set
bc<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",header=F,sep=",")
names(bc)<- c('id_number', 'diagnosis', 'radius_mean', 
              'texture_mean', 'perimeter_mean', 'area_mean', 
              'smoothness_mean', 'compactness_mean', 
              'concavity_mean','concave_points_mean', 
              'symmetry_mean', 'fractal_dimension_mean',
              'radius_se', 'texture_se', 'perimeter_se', 
              'area_se', 'smoothness_se', 'compactness_se', 
              'concavity_se', 'concave_points_se', 
              'symmetry_se', 'fractal_dimension_se', 
              'radius_worst', 'texture_worst', 
              'perimeter_worst', 'area_worst', 
              'smoothness_worst', 'compactness_worst', 
              'concavity_worst', 'concave_points_worst', 
              'symmetry_worst', 'fractal_dimension_worst')
```
```{r basic funcion,message=FALSE, include=FALSE, echo=FALSE} 

# Defining Correlation function
panel.cor <- function(x, y, digits=2, prefix="", cex.cor) 
{
  usr <- par("usr"); on.exit(par(usr)) 
  par(usr = c(0, 1, 0, 1)) 
  r <- abs(cor(x, y)) 
  txt <- format(c(r, 0.123456789), digits=digits)[1] 
  txt <- paste(prefix, txt, sep="") 
  if(missing(cex.cor)) cex <- 0.8/strwidth(txt) 
  
  test <- cor.test(x,y) 
  # borrowed from printCoefmat
  Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, 
                   cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                   symbols = c("***", "**", "*", ".", " ")) 
  
  text(0.5, 0.5, txt, cex = cex * r) 
  text(.8, .8, Signif, cex=cex, col=2) 
}

binomial_deviance <- function(y_obs, yhat){
  epsilon = 0.0001
  yhat = ifelse(yhat < epsilon, epsilon, yhat)
  yhat = ifelse(yhat > 1-epsilon, 1-epsilon, yhat)
  a = ifelse(y_obs==0, 0, y_obs * log(y_obs/yhat))
  b = ifelse(y_obs==1, 0, (1-y_obs) * log((1-y_obs)/(1-yhat)))
  return(2*sum(a + b))
} #prediction accuracy

basic.fit.plots <- function(model) {
  
	# get predicted values
	Predicted <- model$fitted.values
	# get residuals
	Resid <- model$residuals
	# get studentized residuals
	RStudent <- rstudent(model = model)
        # build out a data container from the model
	fit.data = data.frame(
	  'Resid' = Resid,
	  'RStudent' = RStudent,
	  'Predicted' = Predicted
	)
	
	# create qqplot of residuals with reference line
	qqplot.resid <- fit.data %>% 
	  ggplot(aes(sample = Resid)) +
	  geom_qq() + geom_qq_line() +
	  labs(subtitle = 'QQ Plot of Residuals',
	       x = 'Theoretical Quantile',
	       y = 'Acutal Quantile')
	
	# create histogram of residuals
	hist.resid <- fit.data %>% 
	  ggplot(aes(x = Resid)) +
	  geom_histogram(bins = 15) + 
	  labs(subtitle = 'Histogram of Residuals',
	       x = 'Residuals',
	       y = 'Count')

	# create scatter plot of residuals vs
        # predicted values
	resid.vs.pred <- fit.data %>% 
	  ggplot(aes(x = Predicted, y = Resid)) +
	  geom_point() +
	  geom_abline(slope = 0) + 
	  labs(subtitle = 'Residuals vs Predictions',
	       x = 'Predicted Value',
	       y = 'Residual')

	# create scatter plot of studentized 
	# residuals vs predicted values
	rStud.vs.pred <- fit.data %>% 
	  ggplot(aes(x = Predicted, y = RStudent)) +
	  geom_point() +
	  geom_abline(slope = 0) + 
  	  geom_abline(slope = 0, intercept = -2) + 
  	  geom_abline(slope = 0, intercept = 2) + 
	  labs(subtitle = 'RStudent vs Predictions',
	       x = 'Predicted Value',
	       y = 'RStudent')
	
	# add all four plots to grid as
	# qqplot           histogram
	# resid vs pred    RStud vs pred
	grid.arrange(qqplot.resid,
		     hist.resid,
		     resid.vs.pred,
		     rStud.vs.pred, 
		     nrow = 2,
		     top = 'Fit Assessment Plots')
}

#### function ends

```

# 1. Introduction

Breast cancer is the most common cancer among women and one of the major causes of death among women worldwide. Every year approximately 124 out of 100,000 women are diagnosed with breast cancer, and the estimation is that 23 out of the 124 women will die of this disease. When detected in its early stages, there is a 30% chance that the cancer can be treated eﬀectively, but the late detection of advanced-stage tumors makes the treatment more diﬃcult. Currently, the most used techniques to detect breast cancer in early stages are: mammography (63% to 97% correctness), FNA (Fine Needle Aspiration) with visual interpretation (65% to 98% correctness) and surgical biopsy (approximately 100% correctness). Therefore, mammography and FNA with visual interpretation correctness varies widely, and the surgical biopsy, although reliable, is invasive and costly.
[Ref: https://www.cancer.org/cancer/breast-cancer/about/how-common-is-breast-cancer.html]

The breast cancer data set used for analysis in this project has numerous measurements taken from tumor biopsies. The goal of using this data set is to address various objectives of the project and build models to predict using the metrics alone if the biopsy is cancer or not.


# 2. Dataset and Description

The data set used for analysis is available at:
https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data

Observations in the data set are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. The data is comprised of 31 numeric features and one categorical feature, which contain 569 observations. The first two columns gives Sample ID and Classes(diagnosis) which has M, malignant and B, benign breast mass. For each cell nucleaus, the following ten charactoristics were measured: 
Radius, Texture,
Perimeter,
Area,
Smoothness,
Compactness,
Concavity,
Concave points,
Symmetry,
Fractal dimension.
For each characteristic three measures are given:
Mean,
Standard error,
Largest/ “worst”.  
The dataset we chose for this project was a publicly shared, breast cancer Wisconsin(diagnostic) data made available through Kaggle in csv format. 

The dataset consists of 569 rows and 32 attributes. We have both categorical and continous attributes in the dataset. The attributes are summarized as below:  

1. id                      ID number
2. diagnosis               The diagnosis of breast tissues (M = malignant, B = benign)
3. radius_mean             mean of distances from center to points on the perimeter
4. texture_mean            standard deviation of gray-scale values
5. perimeter_mean          mean size of the core tumor
6. area_mean
7. smoothness_mean         mean of local variation in radius lengths
8. compactness_mean        mean of perimeter^2 / area - 1.0
9. concavity_mean          mean of severity of concave portions of the contour
10. concave points_mean     mean for number of concave portions of the contour
11. symmetry_mean
12. fractal_dimension_mean  mean for "coastline approximation" - 1
13. radius_se               standard error for the mean of distances from center to points on the perimeter
14. texture_se              standard error for standard deviation of gray-scale values
15. perimeter_se
16. area_se
17. smoothness_se           standard error for local variation in radius lengths
18. compactness_se          standard error for perimeter^2 / area - 1.0
19. concavity_se            standard error for severity of concave portions of the contour
20. concave points_se       standard error for number of concave portions of the contour
21. symmetry_se
22. fractal_dimension_se    standard error for "coastline approximation" - 1
23. radius_worst            "worst" or largest mean value for mean of distances from center to points on the perimeter
24. texture_worst           "worst" or largest mean value for standard deviation of gray-scale values
25. perimeter_worst
26. area_worst
27. smoothness_worst        "worst" or largest mean value for local variation in radius lengths
28. compactness_worst       "worst" or largest mean value for perimeter^2 / area - 1.0
29. concavity_worst         "worst" or largest mean value for severity of concave portions of the contour
30. concave points_worst    "worst" or largest mean value for number of concave portions of the contour
31. symmetry_worst
32. fractal_dimension_worst "worst" or largest mean value for "coastline approximation" - 1

# 3. Exploratory Data Analysis  

We start Exploratory Data Analysis (EDA) of the dataset by checking the number of observations, structure of each variable and summary statistics:  

## 3-1. Summary statistics and glimpse of the data set  

```{r EDA, message = FALSE, include = FALSE, echo = TRUE}
# glimpse(bc)
bc<-bc %>% dplyr::select(-id_number) #deleting ID
bc$diagnosis <- factor(ifelse(bc$diagnosis == 'B', 0, 1)) # class variable change to 1(M) and 0(B)
glimpse(bc)
```
```{r summary, echo=TRUE}
summary(bc)
```

As seen above, there are no missing values and the Class distribution is: 357 Benign, 212 Malignant. First column represents the sample ID so we deleted the column. Also we changed response variable from M, B to 1 and 0. 

## 3-2. Correlation HeatMap  

Plotting Correlation Heatmap for the data to observe the nature and extent of correlation between various features in the dataset:  
```{r HeatMap, echo = FALSE}
correlator  <-  function(df){
	df %>% 	
		keep(is.numeric) %>%
		cor %>%
		corrplot( addCoef.col = "white", number.digits = 2,
			 number.cex = 0.5, method="circle",
			 order="hclust", title="Variable Corr Heatmap",
			 tl.srt=45, tl.cex = 0.8)
}
correlator(bc)
```

As seen in the heatmap, there is strong correlation between:  
1. Texture_mean and Texture_worst  
2. area_se, radius_se, perimeter_se, area_mean, radius_mean, perimeter_mean, area_worst, radius_worst, perimeter_worst, concave.points_worst, concavity_mean, concave.points_mean  
3. smoothness_mean, smoothness_worst, fractal_dimension_mean, fractal_dimension_worst  
4. compactness_mean, compactness_worst, concavity_worst  
5. symmetry_mean, symmetry_worst  
6. compactness_se, fractal_dimension_se, concavity_se, concavity.points_se  


## 3-3. EDA with various graphics

Of the observations around 40% have Malignant tumor. In malignant group, we see high values on perimeter_mean, convave.point_mean, area_mean.  
```{r EDA graphics, message = FALSE, include = FALSE, echo = FALSE}
## visualize with some feature and response

p1 <- bc %>% ggplot(aes(diagnosis)) + geom_bar()
p2 <- bc %>% ggplot(aes(diagnosis,perimeter_mean)) + 
  geom_jitter(col='gray') +
  geom_boxplot(alpha=.5) # alpha: transparency
  
p3 <- bc %>% ggplot(aes(diagnosis,concave_points_mean)) + 
  geom_jitter(col='orange') +
  geom_boxplot(alpha=.5)

p4 <- bc %>% ggplot(aes(diagnosis,area_mean)) + 
  geom_jitter(col='blue') +
  geom_boxplot(alpha=0.5)

p5 <- bc %>% ggplot(aes(diagnosis,texture_mean)) + 
  geom_jitter(col='red') +
  geom_boxplot(alpha=0.5)

p6 <- bc %>% ggplot(aes(concave_points_mean, radius_mean)) +
  geom_jitter(col='grey') +
  geom_smooth()
```
```{r EDA explain, echo=FALSE, fig.width=10, fig.height=5, message = FALSE}
grid.arrange(p1,p2,p3,p4,p5,p6,ncol=3)
```

## 3-3. Pairwise Plots  

```{r EDA explain pairplot, fig.width=10, fig.height=8, message = FALSE, echo=FALSE}
pairs(bc %>% dplyr::select(diagnosis, ends_with('_mean')) %>% 
        sample_n(min(1000, nrow(bc))),
      lower.panel = function(x,y){points(x,y); abline(0,1,col='red')},
      upper.panel = panel.cor)
```

The pairwise plot also confirms that lots of features are correlated, especially the radius, parameter and area are highly correlated as expected from their relationship. So, from these we can use anyone of them.  

Compactness_mean, concavity_mean and concave point_mean are highly correlated so we can use compactness_mean from here.  

Based on the EDA, we think the following features would be of great importance for use in the analysis:  
 ['texture_mean','perimeter_mean','smoothness_mean','concave_point_mean','symmetry_mean']  
 
 
# 4. Split the available data set into Train, Validate and Test portions:  

For the sake of our analysis we split the available data set into Train, Validate and Test subsets.
```{r data split, message=FALSE, include=FALSE, echo=FALSE}
# data spilit 60:20:20 (training: validate: test)
set.seed(1606)
n <- nrow(bc)
idx <- 1:n
training_idx <- sample(idx, n * .60)
idx <- setdiff(idx, training_idx)
validate_idx <- sample(idx, n * .20)
test_idx <- setdiff(idx, validate_idx)
training <- bc[training_idx,]
validation <- bc[validate_idx,]
test <- bc[test_idx,]
pairs(training[,2:10],col=training$diagnosis)
```

# Objective 1:  
Perform logistic regression analysis and provide interpretation of the regression coefficients including confidence intervals.

# 4-1. Problem Statement:  

The purpose of this analysis is to build a model to predict using the metrics alone (most appropriate risk factors) if the biopsy is cancer or not.  

Model selection techniques will be used and a final model will be selected based on various performance criteria. Interpretation of the selected model will be provided as well.  

## 4-2. Model Selection & Logistic Regression

For the model selection, we use LASSO and manual/intuition models and compare them to see which gives us the best results.  

### Key Assumptions:  

Before running logistic regression we proceed making sure the following two key assumptions are met.  
1. Logistic regression requires the observations to be independent of each other. For this data set we do not have information if any of the observations recorded belonged to members from the same family. And hence we assume that all the observations are independent of each other.  
2. Logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other. During the EDA we identified highly correlated variables and we make sure to remove them when building the models.  


### 4-2-1. Manual/Intuition: Based on EDA  

Based on the EDA, we think the following features would be of great importance for use in the analysis:  
 ['texture_mean','perimeter_mean','smoothness_mean','concave_point_mean','symmetry_mean']  
 
```{r, message = FALSE, include = FALSE, echo = FALSE}
model.check <-glm(diagnosis ~ texture_mean+perimeter_mean+smoothness_mean+concave_points_mean+symmetry_mean, data = training, family = binomial(link = "logit"))
summary(model.check)
```

#### 4-2-1-1. Residual diagnostics  

Plots here help us examine Cook's D graph:
```{r, echo = FALSE}
plot(model.check)
```

When checking Cook's D plot, if observations are outside the Cook's distance (meaning they have a high Cook's distance score) the observations are influential to the regression results. In this case, from the Cook's D plot above, we do not see any influential points.  


#### 4-2-1-2. Lack of fit test  

We then run the Hosmer Lemeshow test to check the lack of fit. And the Hosmer Lemeshow test looks fine since p-value > 0.05. Hence, we conclude that the model is a good fit. 
```{r, echo = FALSE}
hoslem.test(model.check$y, fitted(model.check), g=10) # it is ok

exp(cbind("Odds ratio" = coef(model.check), confint.default(model.check, level = 0.95))) # ODDS ratio
```
```{r validation, message = FALSE, include = FALSE, echo = FALSE}
y_obs<-as.numeric(as.character(validation$diagnosis))
y_hat_lm_check <-predict(model.check, newdata = validation, type = 'response')
binomial_deviance(y_obs, y_hat_lm_check)

pred_lm_check <-prediction(y_hat_lm_check, y_obs)
AUC.lm.check <-performance(pred_lm_check, "auc")@y.values[[1]] #AUC: 0.989
perf_lm_check<-performance(pred_lm_check,measure="tpr", x.measure = "fpr")
```

### 4-2-2. LASSO  

We now try to build the model using LASSO. Since LASSO is a regularized model, binomial deviance can be used for model selection. Figure below shows the binomial deviance. The binomial deviance is minimized near the vertical lines (10: best prediction, 8: most simple to interpret).  

```{r LASSO, message = FALSE, include = FALSE, echo = FALSE}
xx <- model.matrix(diagnosis ~ .-1, bc) #-1: no need of intercept
x <-xx[training_idx,]
y <-as.numeric(as.character(training$diagnosis))
glimpse(x)

## using cv.glmnet() to fit
bc_cvfit<-cv.glmnet(x,y, family = 'binomial')
plot(bc_cvfit)
###fidning: at 10: best prediction(lambda.min), at 7: most simple to interpret(lambda.1se)

coef(bc_cvfit, s = c("lambda.1se"))
coef(bc_cvfit, s = c("lambda.min"))

## validate model

predict.cv.glmnet(bc_cvfit, s="lambda.min", newx = x[1:5,], type = 'response')

yhat_glmnet <-predict(bc_cvfit, s = "lambda.min", newx=xx[validate_idx,], type = 'response')
yhat_glmnet <-yhat_glmnet[,1] # change to a vector from [n*1] matrix
pred_glmnet <-prediction(yhat_glmnet, y_obs)
AUC.glm <-performance(pred_glmnet,"auc")@y.values[[1]]
binomial_deviance(y_obs, yhat_glmnet)
perf_glmnet<-performance(pred_glmnet, measure = "tpr", x.measure = "fpr")
```
```{r LASSO graph}
coef(bc_cvfit, s = c("lambda.1se"))
plot(bc_cvfit)

```


The features selected at the "most simple to interpret" line are: concave_points_mean, radius_se, radius_worst, texture_worst, smoothness_worst, concavity_worst, concave_points_worst, symmetry_worst.

```{r interpretation, echo = TRUE}
model.LASSO <-glm(diagnosis ~ concave_points_mean + radius_se + radius_worst + texture_worst + smoothness_worst + 
                    concavity_worst + concave_points_worst + symmetry_worst,
                  data = training, family = binomial(link = "logit"))

summary(model.LASSO)
confint.default(model.LASSO, level = 0.95)
```

From the above output we see that only the following parameters are **statistically significant**: radius_se, radius_worst, texture_worst and smoothness_worst.  


#### 4-2-2-1. Residual diagnostics  

Plots here help us examine Cook's D graph:
```{r, echo = FALSE}
plot(model.LASSO)
```

When checking Cook's D plot, if observations are outside the Cook's distance (meaning they have a high Cook's distance score) the observations are influential to the regression results. In this case, from the Cook's D plot above, we do not see any influential points.  


#### 4-2-2-2. Lack of fit test  

We then run the Hosmer Lemeshow test to check the lack of fit. And the Hosmer Lemeshow test looks fine since p-value > 0.05. Hence, we conclude that the model is a good fit. 
```{r, echo = FALSE}
hoslem.test(model.LASSO$y, fitted(model.LASSO), g=10) # it is ok

exp(cbind("Odds ratio" = coef(model.LASSO), confint.default(model.LASSO, level = 0.95))) # ODDS ratio

```

### 4-2-3. Model validation  

Below is ROC curve comparison of the two models above. And we see that **LASSO** seems to work better.  

```{r model validation, echo=FALSE}
plot(perf_lm_check, col='black', main="ROC Curve Comparison")
text(x = .40, y = .6,paste("AUC = ", round(AUC.lm.check,5)), col='black')
plot(perf_glmnet, col = 'blue', add = TRUE)
text(x = .40, y = .7,paste("AUC = ", round(AUC.glm,5)), col='blue')

abline(0,1)
legend('bottomright', inset = .1,
       legend = c("LASSO", "model.check"),
       col=c('blue','black'), lty=1, lwd=2)
```

### 4-2-4. Interpretation:  

Based on the Odds ratio output above, interpretations can be made as below for the parameters that are statiscally significant:  

**For radius_se**: For every 0.01 unit increase in radius_se, the odds of diagnosis being Malignant will increase by 17.6% [exp(16.24499 * 0.01)], holding all other explanatories fixed. A 95% confidence interval for the percentage increase is between [ 2.9% [exp(2.8168479 * 0.01)] and 34.5% [exp(29.6731317 * 0.01)] ].  

**For radius_worst**: For every 0.1 unit increase in radius_worst, the odds of diagnosis being Malignant will increase by 15.8% [exp(1.46875 * 0.1)], holding all other explanatories fixed. A 95% confidence interval for the percentage increase is between [ 7.2% [exp(0.6914469 * 0.1)] and 25.2% [exp(2.2460514 * 0.1)] ].  

**For texture_worst**: For every 0.1 unit increase in texture_worst, the odds of diagnosis being Malignant will increase by 3.2% [exp(0.31784 * 0.1)], holding all other explanatories fixed. A 95% confidence interval for the percentage increase is between [ 1.3% [exp(0.1322926 * 0.1)] and 5.2% [exp(0.5033966 * 0.1)] ].  

**For smoothness_worst**: For every 0.001 unit increase in smoothness_worst, the odds of diagnosis being Malignant will increase by 7.4% [exp(71.68613 * 0.001)], holding all other explanatories fixed. A 95% confidence interval for the percentage increase is between [ 0.6% [exp(6.9275128 * 0.001)] and 14.6% [exp(136.4447521 * 0.001)] ].  


### 4-2-3. Building a complex model using interactions  

In order to build a complex model using interactions we use the statistically significant variables derived using LASSO and based on manual intuition add the following interactions: texture_mean:perimeter_mean and smoothness_mean:concave_points_mean.  


```{r complex model, message = FALSE, include = TRUE, echo = TRUE}

model.null<-glm(diagnosis ~ 1, data=training,family = binomial(link="logit"))

model.complex<-glm(diagnosis ~ radius_se+radius_worst+texture_worst+smoothness_worst+                    texture_mean:perimeter_mean+smoothness_mean:concave_points_mean
                   , data=training,family = binomial(link="logit"))

step(model.null,
     scope = list(upper=model.complex),
     direction="forward",
     test="Chisq",
     data=training)

hoslem.test(model.complex$y, fitted(model.complex), g=10)

# validate model.complex
y_obs<-as.numeric(as.character(validation$diagnosis))
y_hat_complex <-predict(model.complex, newdata = validation, type = 'response')
binomial_deviance(y_obs, y_hat_complex)

pred_complex <-prediction(y_hat_complex, y_obs)
AUC.complex <-performance(pred_complex, "auc")@y.values[[1]] #AUC: 0.989
perf_complex<-performance(pred_complex,measure="tpr", x.measure = "fpr")
```

Make note of the changes to the binomial_deviance value by adding interaction terms. At the end of this report we will compare various models. We will also add a graphical comparison at the end of this report. Below is complex model that we built and binomial_deviance. Hosmer Lemeshow test was run to check the lack of fit. And the Hosmer Lemeshow test looks fine since p-value > 0.05. 

```{r complex model_, echo=TRUE}

model.complex<-glm(diagnosis ~ radius_se+radius_worst+texture_worst+smoothness_worst+                    texture_mean:perimeter_mean+smoothness_mean:concave_points_mean
                    , data=training,family = binomial(link="logit"))

binomial_deviance(y_obs, y_hat_complex)
```

### 4-2-3. LDA  

Another competing model was built using all the avilable continuous predictors and LDA. Misclassification error was then calculated as 0.002, in other words, overal accuracy is 0.998. We then check the binomial_deviance of LDA which is 13.51.  

```{r LDA, echo=FALSE}
# LDA 
model.lda<-lda(diagnosis~.,data=training)
pred.lda<-predict(model.lda,newdata=validation)$class
Truth<-validation$diagnosis
x<-table(pred.lda,Truth)
x

#Missclassification Error
ME<-(x[2,1]+x[1,2])/1000
ME

#Calculating overall accuracy
1-ME

y_hat_lda<-predict(model.lda,newdata=validation)$posterior
y_hat_lda<-y_hat_lda[,"1"] # getting y_hat when "1"
pred_lda <- prediction(y_hat_lda, y_obs)
performance(pred_lda, "auc")@y.values[[1]]
binomial_deviance(y_obs, y_hat_lda)

AUC.lda <-performance(pred_lda, "auc")@y.values[[1]]
perf_lda<-performance(pred_lda,measure="tpr", x.measure = "fpr")
```

### 4-2-4. Tree model  

Tree model gives us very simple and intuitive model as shown below. However, it gives us lower accuracy and higher binomial_deviance. We will have random forest model as well later. We will add a graphical comparison at the end of this report.
```{r tree model, message = FALSE, include = FALSE, echo = FALSE}
# Tree model
bc_tr <-rpart(diagnosis ~., data = training)
bc_tr
printcp(bc_tr)
summary(bc_tr)

opar <- par(mfrow=c(1,1), xpd = NA)
plot(bc_tr)
text(bc_tr, use.n = TRUE)
par(opar)
# dev.off()

#validate bc_tr
yhat_tr <- predict(bc_tr, validation)
yhat_tr <- yhat_tr[,"1"]
pred_tr <- prediction(yhat_tr, y_obs)
performance(pred_tr, "auc")@y.values[[1]]
binomial_deviance(y_obs, yhat_tr)
```

```{r tree graph, echo = FALSE}
opar <- par(mfrow=c(1,1), xpd = NA)
plot(bc_tr)
text(bc_tr, use.n = TRUE)
par(opar)
```

### 4-2-5. Random Forest  

Random forest model has better accuarcy than tree model and lower binomial_deviance than tree model. We can confirm the reduction of MSE by adding more trees.  

```{r rf,model, message = FALSE, include = FALSE, echo = FALSE}
# random forest

set.seed(1607)
data_rf <- randomForest(diagnosis ~ ., training)
data_rf

opar <- par(mfrow=c(1,2))
plot(data_rf)
varImpPlot(data_rf)
par(opar)
dev.off()

yhat_rf <- predict(data_rf, newdata=validation, type='prob')[,'1']
pred_rf <- prediction(yhat_rf, y_obs)
AUC_rf <- performance(pred_rf, "auc")@y.values[[1]]
perf_rf <- performance(pred_rf, measure = "tpr", x.measure = "fpr")
binomial_deviance(y_obs, yhat_rf)
```

```{r random forest, fig.height=6, fig.width=10, echo=FALSE}
opar <-par(mfrow = c(1,2))
plot(data_rf)
varImpPlot(data_rf)
par(opar)
```

# 5. Model comparison  

The table below compares various models using AUC and Binomial Deviation values. From the table we can confirm that LASSO has best prediction performance and LDA has least error. From pair plot, we can confirms that LAASO or LDA has best coefficient with observation. Also we see LASSO, complex model (*this was expected*), random forest and LDA are highly correlated (coef: 0.97)  

```{r accuracy comp, echo=FALSE, fig.width=8, fig.height=5}
data.frame(method=c('LASSO','LDA', 'RandomForest', 'Complex', 'Model.check','Tree'),
           auc = c(performance(pred_glmnet, "auc")@y.values[[1]],
                   performance(pred_lda, "auc")@y.values[[1]],
                   performance(pred_rf, "auc")@y.values[[1]],
                   performance(pred_complex, "auc")@y.values[[1]],
                   performance(pred_lm_check, "auc")@y.values[[1]],
                   performance(pred_tr, "auc")@y.values[[1]]
                   ),
           bin_dev = c(binomial_deviance(y_obs, yhat_glmnet),
                       binomial_deviance(y_obs, y_hat_lda),
                       binomial_deviance(y_obs, yhat_rf),
                       binomial_deviance(y_obs, y_hat_complex),
                       binomial_deviance(y_obs, y_hat_lm_check),
                       binomial_deviance(y_obs, yhat_tr)
                       )
                       )
### ROC curve comparison
# png("./plots/1-4.png", 5.5*1.2, 4*1.2, units='in', pointsize=9, res=600)
plot(perf_lm_check, col='black', main="ROC Curve Comparison")
text(x = .40, y = .6,paste("AUC = ", round(AUC.lm.check,5)), col='black')
plot(perf_lda, col='grey',add = TRUE)
text(x = .40, y = .3,paste("AUC = ", round(AUC.lda,5)), col='grey')
plot(perf_glmnet, col = 'blue', add = TRUE)
text(x = .40, y = .7,paste("AUC = ", round(AUC.glm,5)), col='blue')
plot(perf_complex, col ='orange', add = TRUE)
text(x = .40, y = .5,paste("AUC = ", round(AUC.complex,5)), col='orange')
plot(perf_rf, col = "dark green", add=TRUE)
text(x = .40, y = .4,paste("AUC = ", round(AUC_rf,5)), col='dark green')
abline(0,1)
legend('bottomright', inset = .1,
       legend = c("LASSO", "model.check", "complex", "RandomForest", "LDA"),
       col=c('blue','black','orange','dark green','grey'), lty=1, lwd=2)
#pair plot
pairs(data.frame(y_obs = y_obs,
                 yhat_EDA = y_hat_lm_check,
                 yhat_LASSO = c(yhat_glmnet),
                 yhat_complex = y_hat_complex,
                 yhat_rf = yhat_rf,
                 y_hat_lda = y_hat_lda),
      lower.panel = function(x,y){ points(x,y); abline(0, 1, col ='red')},
      upper.panel = panel.cor)
```

# 6. Conclusion  

Based on the analysis, we choose the model from LASSO as the final model, to predict using the metrics alone (most appropriate risk factors) if the biopsy is cancer or not. When we apply this model to new data (test), the performance of LAASO model is seen as: binomial deviance: 14.98, AUC: 0.998  

```{r}
y_obs_test <- as.numeric(as.character(test$diagnosis))
yhat_glmnet_test <-predict(bc_cvfit, s="lambda.min", newx = xx[test_idx,], type = "response")
yhat_glmnet_test <- yhat_glmnet_test[,1]
pred_glmnet_test <- prediction(yhat_glmnet_test, y_obs_test)
performance(pred_glmnet_test, "auc")@y.values[[1]]
binomial_deviance(y_obs_test, yhat_glmnet_test)
```

# APPENDIX  

All the code used to generate this report and complete the analysis is included below:  
