hoslem.test(model.complex$y, fitted(model.complex), g=10)
###ND test
model.main<-glm(diagnosis ~ ., data=bc,family = binomial(link="logit"))
model.null<-glm(diagnosis ~ 1, data=bc,family = binomial(link="logit"))
step(model.null,
scope = list(upper=model.main),
direction="forward",
test="Chisq",
data=bc)
####
#This feature selection included some of the interaction terms.  However their coeffcients are quite small, so we might
#consider this model in terms of helping predictions out, but for interpretation, its going to be more challenging describing effects
#in a more complicated way, but in the end will still look very similar.
#validate model.complex
y_obs<-as.numeric(as.character(validation$diagnosis))
y_hat_complex <-predict(model.complex, newdata = validation, type = 'response')
binomial_deviance(y_obs, y_hat_complex)
pred_complex <-prediction(y_hat_complex, y_obs)
AUC.complex <-performance(pred_complex, "auc")@y.values[[1]] #AUC: 0.989
perf_complex<-performance(pred_complex,measure="tpr", x.measure = "fpr")
# complex model : getting features from EDA: texture_mean+perimeter_mean+smoothness_mean+concave_points_mean+symmetry_mean : chosen from EDA
step(model.check,
scope = list(upper=model.check),
direction="forward",
test="Chisq",
data=training)
model.complex<-glm(diagnosis ~ texture_mean+perimeter_mean+smoothness_mean+concave_points_mean+symmetry_mean+
texture_mean:perimeter_mean+smoothness_mean:concave_points_mean
, data=training,family = binomial(link="logit"))
step(model.check,
scope = list(upper=model.complex),
direction="forward",
test="Chisq",
data=training)
hoslem.test(model.complex$y, fitted(model.complex), g=10)
###ND test
model.main<-glm(diagnosis ~ ., data=bc,family = binomial(link="logit"))
model.null<-glm(diagnosis ~ 1, data=bc,family = binomial(link="logit"))
step(model.null,
scope = list(upper=model.main),
direction="forward",
test="Chisq",
data=bc)
####
#This feature selection included some of the interaction terms.  However their coeffcients are quite small, so we might
#consider this model in terms of helping predictions out, but for interpretation, its going to be more challenging describing effects
#in a more complicated way, but in the end will still look very similar.
#validate model.complex
y_obs<-as.numeric(as.character(validation$diagnosis))
y_hat_complex <-predict(model.complex, newdata = validation, type = 'response')
binomial_deviance(y_obs, y_hat_complex)
pred_complex <-prediction(y_hat_complex, y_obs)
AUC.complex <-performance(pred_complex, "auc")@y.values[[1]] #AUC: 0.989
perf_complex<-performance(pred_complex,measure="tpr", x.measure = "fpr")
# complex model : getting features from EDA: texture_mean+perimeter_mean+smoothness_mean+concave_points_mean+symmetry_mean : chosen from EDA
step(model.check,
scope = list(upper=model.check),
direction="forward",
test="Chisq",
data=training)
model.complex<-glm(diagnosis ~ texture_mean+perimeter_mean+smoothness_mean+concave_points_mean+symmetry_mean+
texture_mean:perimeter_mean+smoothness_mean:concave_points_mean
, data=training,family = binomial(link="logit"))
step(model.check,
scope = list(upper=model.complex),
direction="forward",
test="Chisq",
data=training)
hoslem.test(model.complex$y, fitted(model.complex), g=10)
###ND test
# model.main<-glm(diagnosis ~ ., data=bc,family = binomial(link="logit"))
#
# model.null<-glm(diagnosis ~ 1, data=bc,family = binomial(link="logit"))
#
# step(model.null,
#      scope = list(upper=model.main),
#      direction="forward",
#      test="Chisq",
#      data=bc)
####
#This feature selection included some of the interaction terms.  However their coeffcients are quite small, so we might
#consider this model in terms of helping predictions out, but for interpretation, its going to be more challenging describing effects
#in a more complicated way, but in the end will still look very similar.
#validate model.complex
y_obs<-as.numeric(as.character(validation$diagnosis))
y_hat_complex <-predict(model.complex, newdata = validation, type = 'response')
binomial_deviance(y_obs, y_hat_complex)
pred_complex <-prediction(y_hat_complex, y_obs)
AUC.complex <-performance(pred_complex, "auc")@y.values[[1]] #AUC: 0.989
perf_complex<-performance(pred_complex,measure="tpr", x.measure = "fpr")
binomial_deviance(y_obs, y_hat_lm_check)
binomial_deviance(y_obs, yhat_tr)
binomial_deviance(y_obs, yhat_rf)
knitr::opts_chunk$set(echo = TRUE)
# Importing required libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(glmnet)
library(randomForest)
library(gbm)
library(rpart)
library(boot)
library(data.table)
library(ROCR)
library(gridExtra)
library(ResourceSelection)
# library imports
library(tidyverse)
# Date manipulation
library(lubridate)
# Plotting
library(olsrr)
# RMLSE
library(MLmetrics)
# Correlation Matrix
library(ggcorrplot)
# Lasso
library(glmnet)
# AIC commands
library(MASS)
#library(devtools)
library(tidyverse)
library("tree")
library(gridExtra)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(e1071)
library(ggthemes)
library(caret)
library(tidyverse)
library(scales)
library("openxlsx")
# Importing the data set
bc<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",header=F,sep=",")
knitr::opts_chunk$set(echo = TRUE)
# Importing required libraries
library(dplyr)
library(ggplot2)
library(MASS)
library(glmnet)
library(randomForest)
library(gbm)
library(rpart)
library(boot)
library(data.table)
library(ROCR)
library(gridExtra)
library(ResourceSelection)
# library imports
library(tidyverse)
# Date manipulation
library(lubridate)
# Plotting
library(olsrr)
# RMLSE
library(MLmetrics)
# Correlation Matrix
library(ggcorrplot)
# Lasso
library(glmnet)
# AIC commands
library(MASS)
#library(devtools)
library(tidyverse)
library("tree")
library(gridExtra)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(e1071)
library(ggthemes)
library(caret)
library(tidyverse)
library(scales)
library("openxlsx")
# Importing the data set
bc<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",header=F,sep=",")
names(bc)<- c('id_number', 'diagnosis', 'radius_mean',
'texture_mean', 'perimeter_mean', 'area_mean',
'smoothness_mean', 'compactness_mean',
'concavity_mean','concave_points_mean',
'symmetry_mean', 'fractal_dimension_mean',
'radius_se', 'texture_se', 'perimeter_se',
'area_se', 'smoothness_se', 'compactness_se',
'concavity_se', 'concave_points_se',
'symmetry_se', 'fractal_dimension_se',
'radius_worst', 'texture_worst',
'perimeter_worst', 'area_worst',
'smoothness_worst', 'compactness_worst',
'concavity_worst', 'concave_points_worst',
'symmetry_worst', 'fractal_dimension_worst')
# Defining Correlation function
panel.cor <- function(x, y, digits=2, prefix="", cex.cor)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r <- abs(cor(x, y))
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
test <- cor.test(x,y)
# borrowed from printCoefmat
Signif <- symnum(test$p.value, corr = FALSE, na = FALSE,
cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
symbols = c("***", "**", "*", ".", " "))
text(0.5, 0.5, txt, cex = cex * r)
text(.8, .8, Signif, cex=cex, col=2)
}
binomial_deviance <- function(y_obs, yhat){
epsilon = 0.0001
yhat = ifelse(yhat < epsilon, epsilon, yhat)
yhat = ifelse(yhat > 1-epsilon, 1-epsilon, yhat)
a = ifelse(y_obs==0, 0, y_obs * log(y_obs/yhat))
b = ifelse(y_obs==1, 0, (1-y_obs) * log((1-y_obs)/(1-yhat)))
return(2*sum(a + b))
} #prediction accuracy
basic.fit.plots <- function(model) {
# get predicted values
Predicted <- model$fitted.values
# get residuals
Resid <- model$residuals
# get studentized residuals
RStudent <- rstudent(model = model)
# build out a data container from the model
fit.data = data.frame(
'Resid' = Resid,
'RStudent' = RStudent,
'Predicted' = Predicted
)
# create qqplot of residuals with reference line
qqplot.resid <- fit.data %>%
ggplot(aes(sample = Resid)) +
geom_qq() + geom_qq_line() +
labs(subtitle = 'QQ Plot of Residuals',
x = 'Theoretical Quantile',
y = 'Acutal Quantile')
# create histogram of residuals
hist.resid <- fit.data %>%
ggplot(aes(x = Resid)) +
geom_histogram(bins = 15) +
labs(subtitle = 'Histogram of Residuals',
x = 'Residuals',
y = 'Count')
# create scatter plot of residuals vs
# predicted values
resid.vs.pred <- fit.data %>%
ggplot(aes(x = Predicted, y = Resid)) +
geom_point() +
geom_abline(slope = 0) +
labs(subtitle = 'Residuals vs Predictions',
x = 'Predicted Value',
y = 'Residual')
# create scatter plot of studentized
# residuals vs predicted values
rStud.vs.pred <- fit.data %>%
ggplot(aes(x = Predicted, y = RStudent)) +
geom_point() +
geom_abline(slope = 0) +
geom_abline(slope = 0, intercept = -2) +
geom_abline(slope = 0, intercept = 2) +
labs(subtitle = 'RStudent vs Predictions',
x = 'Predicted Value',
y = 'RStudent')
# add all four plots to grid as
# qqplot           histogram
# resid vs pred    RStud vs pred
grid.arrange(qqplot.resid,
hist.resid,
resid.vs.pred,
rStud.vs.pred,
nrow = 2,
top = 'Fit Assessment Plots')
}
#### function ends
# glimpse(bc)
bc<-bc %>% dplyr::select(-id_number) #deleting ID
bc$diagnosis <- factor(ifelse(bc$diagnosis == 'B', 0, 1)) # class variable change to 1(M) and 0(B)
glimpse(bc)
summary(bc)
correlator  <-  function(df){
df %>%
keep(is.numeric) %>%
cor %>%
corrplot( addCoef.col = "white", number.digits = 2,
number.cex = 0.5, method="circle",
order="hclust", title="Variable Corr Heatmap",
tl.srt=45, tl.cex = 0.8)
}
correlator(bc)
## visualize with some feature and response
p1 <- bc %>% ggplot(aes(diagnosis)) + geom_bar()
p2 <- bc %>% ggplot(aes(diagnosis,perimeter_mean)) +
geom_jitter(col='gray') +
geom_boxplot(alpha=.5) # alpha: transparency
p3 <- bc %>% ggplot(aes(diagnosis,concave_points_mean)) +
geom_jitter(col='orange') +
geom_boxplot(alpha=.5)
p4 <- bc %>% ggplot(aes(diagnosis,area_mean)) +
geom_jitter(col='blue') +
geom_boxplot(alpha=0.5)
p5 <- bc %>% ggplot(aes(diagnosis,texture_mean)) +
geom_jitter(col='red') +
geom_boxplot(alpha=0.5)
p6 <- bc %>% ggplot(aes(concave_points_mean, radius_mean)) +
geom_jitter(col='grey') +
geom_smooth()
grid.arrange(p1,p2,p3,p4,p5,p6,ncol=3)
pairs(bc %>% dplyr::select(diagnosis, ends_with('_mean')) %>%
sample_n(min(1000, nrow(bc))),
lower.panel = function(x,y){points(x,y); abline(0,1,col='red')},
upper.panel = panel.cor)
# data spilit 60:20:20 (training: validate: test)
set.seed(1606)
n <- nrow(bc)
idx <- 1:n
training_idx <- sample(idx, n * .60)
idx <- setdiff(idx, training_idx)
validate_idx <- sample(idx, n * .20)
test_idx <- setdiff(idx, validate_idx)
training <- bc[training_idx,]
validation <- bc[validate_idx,]
test <- bc[test_idx,]
pairs(training[,2:10],col=training$diagnosis)
model.check <-glm(diagnosis ~ texture_mean+perimeter_mean+smoothness_mean+concave_points_mean+symmetry_mean, data = training, family = binomial(link = "logit"))
summary(model.check)
plot(model.check)
hoslem.test(model.check$y, fitted(model.check), g=10) # it is ok
exp(cbind("Odds ratio" = coef(model.check), confint.default(model.check, level = 0.95))) # ODDS ratio
y_obs<-as.numeric(as.character(validation$diagnosis))
y_hat_lm_check <-predict(model.check, newdata = validation, type = 'response')
binomial_deviance(y_obs, y_hat_lm_check)
pred_lm_check <-prediction(y_hat_lm_check, y_obs)
AUC.lm.check <-performance(pred_lm_check, "auc")@y.values[[1]] #AUC: 0.989
perf_lm_check<-performance(pred_lm_check,measure="tpr", x.measure = "fpr")
xx <- model.matrix(diagnosis ~ .-1, bc) #-1: no need of intercept
x <-xx[training_idx,]
y <-as.numeric(as.character(training$diagnosis))
glimpse(x)
## using cv.glmnet() to fit
bc_cvfit<-cv.glmnet(x,y, family = 'binomial')
plot(bc_cvfit)
###fidning: at 10: best prediction(lambda.min), at 7: most simple to interpret(lambda.1se)
coef(bc_cvfit, s = c("lambda.1se"))
coef(bc_cvfit, s = c("lambda.min"))
## validate model
predict.cv.glmnet(bc_cvfit, s="lambda.min", newx = x[1:5,], type = 'response')
yhat_glmnet <-predict(bc_cvfit, s = "lambda.min", newx=xx[validate_idx,], type = 'response')
yhat_glmnet <-yhat_glmnet[,1] # change to a vector from [n*1] matrix
pred_glmnet <-prediction(yhat_glmnet, y_obs)
AUC.glm <-performance(pred_glmnet,"auc")@y.values[[1]]
binomial_deviance(y_obs, yhat_glmnet)
perf_glmnet<-performance(pred_glmnet, measure = "tpr", x.measure = "fpr")
coef(bc_cvfit, s = c("lambda.1se"))
plot(bc_cvfit)
model.LASSO <-glm(diagnosis ~ concave_points_mean + radius_se + radius_worst + texture_worst + smoothness_worst +
concavity_worst + concave_points_worst + symmetry_worst,
data = training, family = binomial(link = "logit"))
summary(model.LASSO)
confint.default(model.LASSO, level = 0.95)
plot(model.LASSO)
hoslem.test(model.LASSO$y, fitted(model.LASSO), g=10) # it is ok
exp(cbind("Odds ratio" = coef(model.LASSO), confint.default(model.LASSO, level = 0.95))) # ODDS ratio
plot(perf_lm_check, col='black', main="ROC Curve Comparison")
text(x = .40, y = .6,paste("AUC = ", round(AUC.lm.check,5)), col='black')
plot(perf_glmnet, col = 'blue', add = TRUE)
text(x = .40, y = .7,paste("AUC = ", round(AUC.glm,5)), col='blue')
abline(0,1)
legend('bottomright', inset = .1,
legend = c("LASSO", "model.check"),
col=c('blue','black'), lty=1, lwd=2)
model.null<-glm(diagnosis ~ 1, data=training,family = binomial(link="logit"))
model.complex<-glm(diagnosis ~ radius_se+radius_worst+texture_worst+smoothness_worst+                    texture_mean:perimeter_mean+smoothness_mean:concave_points_mean
, data=training,family = binomial(link="logit"))
step(model.null,
scope = list(upper=model.complex),
direction="forward",
test="Chisq",
data=training)
hoslem.test(model.complex$y, fitted(model.complex), g=10)
###ND test
# model.main<-glm(diagnosis ~ ., data=bc,family = binomial(link="logit"))
#
# model.null<-glm(diagnosis ~ 1, data=bc,family = binomial(link="logit"))
#
# step(model.null,
#      scope = list(upper=model.main),
#      direction="forward",
#      test="Chisq",
#      data=bc)
####
#This feature selection included some of the interaction terms.  However their coeffcients are quite small, so we might
#consider this model in terms of helping predictions out, but for interpretation, its going to be more challenging describing effects
#in a more complicated way, but in the end will still look very similar.
#validate model.complex
y_obs<-as.numeric(as.character(validation$diagnosis))
y_hat_complex <-predict(model.complex, newdata = validation, type = 'response')
binomial_deviance(y_obs, y_hat_complex)
pred_complex <-prediction(y_hat_complex, y_obs)
AUC.complex <-performance(pred_complex, "auc")@y.values[[1]] #AUC: 0.989
perf_complex<-performance(pred_complex,measure="tpr", x.measure = "fpr")
# model.complex<-glm(diagnosis ~ radius_se+radius_worst+texture_worst+smoothness_worst+                    texture_mean:perimeter_mean+smoothness_mean:concave_points_mean
#                    , data=training,family = binomial(link="logit"))
binomial_deviance(y_obs, y_hat_complex)
# LDA
model.lda<-lda(diagnosis~.,data=training)
pred.lda<-predict(model.lda,newdata=validation)$class
Truth<-validation$diagnosis
x<-table(pred.lda,Truth)
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/1000
ME
#Calculating overall accuracy
1-ME
y_hat_lda<-predict(model.lda,newdata=validation)$posterior
y_hat_lda<-y_hat_lda[,"1"] # getting y_hat when "1"
pred_lda <- prediction(y_hat_lda, y_obs)
performance(pred_lda, "auc")@y.values[[1]]
binomial_deviance(y_obs, y_hat_lda)
AUC.lda <-performance(pred_lda, "auc")@y.values[[1]]
perf_lda<-performance(pred_lda,measure="tpr", x.measure = "fpr")
# Tree model
bc_tr <-rpart(diagnosis ~., data = training)
bc_tr
printcp(bc_tr)
summary(bc_tr)
opar <- par(mfrow=c(1,1), xpd = NA)
plot(bc_tr)
text(bc_tr, use.n = TRUE)
par(opar)
# dev.off()
#validate bc_tr
yhat_tr <- predict(bc_tr, validation)
yhat_tr <- yhat_tr[,"1"]
pred_tr <- prediction(yhat_tr, y_obs)
performance(pred_tr, "auc")@y.values[[1]]
binomial_deviance(y_obs, yhat_tr)
opar <- par(mfrow=c(1,1), xpd = NA)
plot(bc_tr)
text(bc_tr, use.n = TRUE)
par(opar)
# random forest
set.seed(1607)
data_rf <- randomForest(diagnosis ~ ., training)
data_rf
opar <- par(mfrow=c(1,2))
plot(data_rf)
varImpPlot(data_rf)
par(opar)
dev.off()
yhat_rf <- predict(data_rf, newdata=validation, type='prob')[,'1']
pred_rf <- prediction(yhat_rf, y_obs)
AUC_rf <- performance(pred_rf, "auc")@y.values[[1]]
perf_rf <- performance(pred_rf, measure = "tpr", x.measure = "fpr")
binomial_deviance(y_obs, yhat_rf)
opar <-par(mfrow = c(1,2))
plot(data_rf)
varImpPlot(data_rf)
par(opar)
data.frame(method=c('LASSO','LDA', 'RandomForest', 'Complex', 'Model.check','Tree'),
auc = c(performance(pred_glmnet, "auc")@y.values[[1]],
performance(pred_lda, "auc")@y.values[[1]],
performance(pred_rf, "auc")@y.values[[1]],
performance(pred_complex, "auc")@y.values[[1]],
performance(pred_lm_check, "auc")@y.values[[1]],
performance(pred_tr, "auc")@y.values[[1]]
),
bin_dev = c(binomial_deviance(y_obs, yhat_glmnet),
binomial_deviance(y_obs, y_hat_lda),
binomial_deviance(y_obs, yhat_rf),
binomial_deviance(y_obs, y_hat_complex),
binomial_deviance(y_obs, y_hat_lm_check),
binomial_deviance(y_obs, yhat_tr)
)
)
### ROC curve comparison
# png("./plots/1-4.png", 5.5*1.2, 4*1.2, units='in', pointsize=9, res=600)
plot(perf_lm_check, col='black', main="ROC Curve Comparison")
text(x = .40, y = .6,paste("AUC = ", round(AUC.lm.check,5)), col='black')
plot(perf_lda, col='grey',add = TRUE)
text(x = .40, y = .3,paste("AUC = ", round(AUC.lda,5)), col='grey')
plot(perf_glmnet, col = 'blue', add = TRUE)
text(x = .40, y = .7,paste("AUC = ", round(AUC.glm,5)), col='blue')
plot(perf_complex, col ='orange', add = TRUE)
text(x = .40, y = .5,paste("AUC = ", round(AUC.complex,5)), col='orange')
plot(perf_rf, col = "dark green", add=TRUE)
text(x = .40, y = .4,paste("AUC = ", round(AUC_rf,5)), col='dark green')
abline(0,1)
legend('bottomright', inset = .1,
legend = c("LASSO", "model.check", "complex", "RandomForest", "LDA"),
col=c('blue','black','orange','dark green','grey'), lty=1, lwd=2)
#pair plot
pairs(data.frame(y_obs = y_obs,
yhat_EDA = y_hat_lm_check,
yhat_LASSO = c(yhat_glmnet),
yhat_complex = y_hat_complex,
yhat_rf = yhat_rf,
y_hat_lda = y_hat_lda),
lower.panel = function(x,y){ points(x,y); abline(0, 1, col ='red')},
upper.panel = panel.cor)
y_obs_test <- as.numeric(as.character(test$diagnosis))
yhat_glmnet_test <-predict(bc_cvfit, s="lambda.min", newx = xx[test_idx,], type = "response")
yhat_glmnet_test <- yhat_glmnet_test[,1]
pred_glmnet_test <- prediction(yhat_glmnet_test, y_obs_test)
performance(pred_glmnet_test, "auc")@y.values[[1]]
binomial_deviance(y_obs_test, yhat_glmnet_test)
